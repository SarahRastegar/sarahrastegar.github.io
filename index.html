<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Sarah Rastegar</title>
  
  <meta name="author" content="Sarah Rastegar">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-site-verification" content="1W4PlhCZojx9K0LBDmu-lyfMGrpkS2hfGICYyqYP73o" />
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/logo.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Sarah Rastegar</name>
              </p>
              <p> Hello, I am a Ph.D. candidate at the University of Amsterdam (<a href=" https://www.uva.nl/">UvA</a>), privileged to be under the supervision of Prof. <a href="https://www.ceessnoek.info/">Cees Snoek</a>.
                 Additionally, I am fortunate to receive insightful guidance from Prof. <a href="https://yukimasano.github.io/">Yuki Asano</a>. 
                  <br>Before embarking on my Ph.D., I pursued my Master's degree at <a href="https://www.sharif.edu/">Sharif University of Technology</a>, Tehran, Iran, where I was honored to work under the supervision of Dr. <a href="https://sharif.edu/~soleymani/">Mahdieh Soleymani Baghshah</a>.</p>
                  My Master's thesis, titled <em>Deep Learning for Multimodal Data</em>, delved into the intricate layers of multimodal data through the lens of knowledge distillation. View my <a href="http://library.sharif.ir/parvan/resource/420659/%DB%8C%D8%A7%D8%AF%DA%AF%DB%8C%D8%B1%DB%8C-%DA%98%D8%B1%D9%81-%D8%A8%D8%B1%D8%A7%DB%8C-%D8%AF%D8%A7%D8%AF%D9%87-%D9%87%D8%A7%DB%8C-%DA%86%D9%86%D8%AF%DA%AF%D9%88%D9%86%D9%87/preview/55021/%D9%85%D8%AD%D8%AA%D9%88%D8%A7%D9%8A-%DA%A9%D8%AA%D8%A7%D8%A8">thesis</a> (in Persian).<br>
                  <br>My research focuses on enabling machine learning models to exhibit robust generalization to unseen domains, particularly leveraging self-supervision. During my Masterâ€™s, I explored transferring knowledge between modalities through multimodal learning. My Ph.D. journey has navigated through utilizing causal inference and Fourier frequency filtering to facilitate domain generalization. Presently, my research is focused on developing a framework that empowers models to discern novel categories, predominantly through self-supervision.
              <p style="text-align:center">
                <a href="mailto:s.rastegar2@uva.nl">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=e_HGE3gAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/SarahRastegar">Github</a> &nbsp/&nbsp 
                <a href="https://www.linkedin.com/in/sarah-rastegar/">Linkedin</a> &nbsp&nbsp 

                
              </p>

            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/profileC.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profileC.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading></heading>
              <p>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td></td>
              <heading>News</heading>
            </td>
            </tr>
            <tr>
              <td> <strong>July 2024</strong> </td>
              <td><strong>SelEx: Self-Expertise in Fine-Grained Generalized Category Discovery</strong> got accepted to ECCV. </td>
            </tr>
            <tr>
              <td> <strong>February 2024</strong> </td>
              <td><strong>Background No More: Action Recognition Across Domains by Causal Interventions</strong> got accepted to CVIU. </td>
            </tr>
            <tr>
              <td> <strong>September 2023</strong> </td>
              <td><strong>Learn to categorize or categorize to learn?</strong> got accepted to NeurIPS. </td>
            </tr>
            <tr>
              <td> <strong>July 2022</strong> </td>
              <td>I attended the 12th Lisbon Machine Learning School (<a href=http://lxmls.it.pt/2022/>LxMLS</a>) in Lisbon, Portugal. </td>
            </tr>
            <tr>
              <td> <strong>September 2021</strong></td>
              <td> I became <strong>Soos Chair</strong> for <a href=" https://ivi.fnwi.uva.nl/vislab/">VisLab</a> weekly Soos meetings.</td>
          </tr>
          <tr>
            <td> <strong>September 2021</strong> </td>
            <td>I was a co-organizer for IPM fifth summer school <a href=https://cs.ipm.ac.ir/asoc2021>ASOC2021</a>. </td>
          </tr>
            <tr><td> <strong>November 2020</strong></td>
                <td>I started my Ph.D. in <a href=" https://ivi.fnwi.uva.nl/vislab/">VisLab</a> under supervision of <a href="https://www.ceessnoek.info/">Cees Snoek</a>.</td>
            </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/selex.png" alt="blind-date" width="200" height="120">
            </td>
            <td width="75%" valign="middle">
                <papertitle><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Rastegar_MDL-CW_A_Multimodal_CVPR_2016_paper.pdf">
                  SelEx: Self-Expertise in Fine-Grained Generalized Category Discovery</a>
                </papertitle>
              </a>
              <br>
              <u>Sarah Rastegar</u>, Mohammadreza Salehi, Yuki M Asano, Hazel Doughty, Cees G.M. Snoek
              <br>
              <em>ECCV</em>, 2024
          [<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Rastegar_MDL-CW_A_Multimodal_CVPR_2016_paper.pdf">Arxiv</a>]
              [<a href="https://github.com/SarahRastegar/SelEx">Code</a>]
                <p>In this paper, we introduce a novel concept called <em>self-expertise</em>, which enhances the model's ability to recognize subtle differences and uncover unknown categories. Our approach combines unsupervised and supervised self-expertise strategies to refine the model's discernment and generalization. Our supervised technique differs from traditional methods by utilizing more abstract positive and negative samples, aiding in the formation of clusters that can generalize to novel categories. Meanwhile, our unsupervised strategy encourages the model to sharpen its category distinctions by considering within-category examples as `hard' negatives. Supported by theoretical insights, our empirical results showcase that our method outperforms existing state-of-the-art techniques in Generalized Category Discovery across several fine-grained datasets. </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Infosieve.png" alt="blind-date" width="200" height="120">
            </td>
            <td width="75%" valign="middle">
                <papertitle><a href="https://openreview.net/forum?id=m0vfXMrLwF">Learn to Categorize or Categorize to Learn? Self-Coding for Generalized Category Discovery</a>
                </papertitle>
              </a>
              <br>
              <u>Sarah Rastegar</u>, Hazel Doughty, Cees G.M. Snoek
              <br>
              <em>NeurIPS</em>, 2023
              <br>
                [<a href="https://arxiv.org/abs/2310.19776">Arxiv</a>][<a href="https://github.com/SarahRastegar/InfoSieve">Code</a>]
              <br>
                <p>What exactly delineates a <em>category</em>? In this paper, we conceptualize a <em>category</em> through the lens of optimization, viewing it as an optimal solution to a well-defined problem. Harnessing this unique conceptualization, we propose a novel, efficient and self-supervised method capable of discovering previously unknown categories at test time. A salient feature of our approach is the assignment of minimum length category codes to individual data instances, which encapsulates the implicit category hierarchy prevalent in real-world datasets </p>
            </td>
          </tr>

            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/backgroundnomore.jpg" alt="blind-date" width="200" height="100">
            </td>
            <td width="75%" valign="middle">
                <papertitle>Background No More: Action Recognition Across Domains by Causal Interventions</papertitle>
              </a>
              <br>
              <u>Sarah Rastegar</u>, Hazel Doughty, Cees G.M. Snoek
              <br>
              <em>CVIU</em>, 2024
              <br>
                [<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4327719">SSRN</a>]
              <br>
                <p> We aim to recognize actions under an appearance distribution-shift between a source training-domain
                  and target test-domain. To enable such video domain generalization, our key idea is to intervene on
                  the action to remove the confounding effect of the domain-background on the class label using causal
                  inference. Towards this, we propose to learn a causally debiased model on a source domain that in-
                  tervenes on the action through three possible Do-operators which separate the action and background.
                  To better align the source and target distributions we also introduce a test-time action intervention. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/CVPR16.png" alt="blind-date" width="200" height="120">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/Rastegar_MDL-CW_A_Multimodal_CVPR_2016_paper.pdf">
                <papertitle>MDL-CW: A Multimodal Deep Learning Framework with Cross Weights</papertitle>
              </a>
              <br>
              <em></em><u>Sarah Rastegar</u></em>, Mahdieh Soleymani Baghshah, Hamid R. Rabiee, Seyed Mohsen Shojaee
              <br>
              <em>CVPR</em>, 2016
              <br>
               
              [<a href="https://github.com/SarahRastegar/MDL-CW">Code</a>]
              [<a href="data/cvpr2016.bib">Bibtex</a>] 
              <p>An ideal model for multimodal data
                can reason about missing modalities using the available
                ones, and usually provides more information when multiple
                modalities are being considered. All the previous deep mod-
                els contain separate modality-specific networks and find a
                shared representation on top of those networks. Therefore,
                they only consider high level interactions between modal-
                ities to find a joint representation for them. In this paper,
                we propose a multimodal deep learning framework (MDL-
                CW) that exploits the cross weights between representation
                of modalities, and try to gradually learn interactions of the
                modalities in a deep network manner from low to high level
                interactions. </p>
            </td>
          </tr>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <heading>Teaching Experience</heading>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/logo.png" width="50" height="50"></td>
              <td width="75%" valign="center">
                Fall 2022 - Teaching Assistant for Deep Learning I, <em>Prof. Yuki Asano</em>.
               <br>Fall 2021 - Teaching Assistant for Deep Learning I, <em>Prof. Xiantong Zhen</em>. 
               <br>Fall 2021 - Teaching Assistant for Computer Vision 2, <em>Prof. Shaodi You</em>.
              </td>
          </tbody></table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Awards and Honors</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/INSF.jpeg" width="100" height="50"></td>
            <td width="75%" valign="center">
             Awarded Research grant by Iran National
                  Science Foundation (INSF), for fulfilling
                  undergoing research in the field of Deep
                  Learning</td>
            </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/nokhbe.png" width="100" height="120"></td>
              <td width="75%" valign="center">
                Selected as a National Scientific Elite by the
                Iranian National Elites Foundation, for the
                outstanding academic success</td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/sharif.png" width="100" height="100"></td>
                <td width="75%" valign="center">
                  <strong>3rd Rank</strong> in M.Sc. artificial intelligence
                      graduates in sharif university of technology</td>
                </tr>
            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/sanjesh.jpg" width="100" height="100"></td>
            <td width="75%" valign="center">
              <strong>1st Rank</strong> among 1015 test takers in Nation-Wide Ph.D. Entrance Exam in Computer Engineering - Artificial Intelligence
              <br><strong>1st Rank</strong> among 32,276 test takers in the Nation-Wide M.Sc. Entrance Exam in Artificial Intelligence
              <br><strong>2nd Rank</strong> among 32,276 test takers in the Nation-Wide M.Sc. Entrance Exam in Computer Engineering
              <br><strong>2nd Rank</strong> among 28,293 test takers in the Nation-Wide M.Sc. Entrance Exam in Information Technology Engineering, Computer Networks and Information Security
            
            </td>
          </tr>




        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:left;font-size:small;">
                Template from <a href="https://jonbarron.info/">Jon Barron</a>. 
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
